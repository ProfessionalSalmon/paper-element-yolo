{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ba7f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.163 ðŸš€ Python-3.12.3 torch-2.7.1+cu126 CPU (13th Gen Intel Core(TM) i7-1360P)\n",
      "Setup complete âœ… (16 CPUs, 7.6 GB RAM, 152.6/1006.9 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab3d4c",
   "metadata": {},
   "source": [
    "### Load dataset from Roboflow workspace\n",
    "1. Go to your Roboflow dashboard: https://app.roboflow.com/\n",
    "2. Look at the URL when you're in your project\n",
    "3. The URL format is: https://app.roboflow.com/[WORKSPACE_NAME]/[PROJECT_NAME]/[VERSION]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedd1376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dataset downloaded to: /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1\n",
      "Dataset YAML file: /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/data.yaml\n"
     ]
    }
   ],
   "source": [
    "# Download your dataset from Roboflow\n",
    "import roboflow\n",
    "\n",
    "# Initialize Roboflow with your API key\n",
    "rf = roboflow.Roboflow(api_key=\"f5NyplR2eZarG2ts7Ai5\")\n",
    "\n",
    "workspace_name = \"tokyo-fjwy4\"  # e.g., \"john-doe\" or \"my-company\"\n",
    "project_name = \"tft-id-cjkvr\"  # Your project name\n",
    "version_number = \"1\"  # Your version number\n",
    "\n",
    "# Get your project and dataset\n",
    "project = rf.workspace(workspace_name).project(project_name)\n",
    "dataset = project.version(version_number).download(\"yolov11\")\n",
    "\n",
    "# The dataset will be downloaded to a folder with metadata\n",
    "print(f\"Dataset downloaded to: {dataset.location}\")\n",
    "print(f\"Dataset YAML file: {dataset.location}/data.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40528772",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a845d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO with your custom dataset\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pre-trained YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")  # nano (fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b104f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.163 ðŸš€ Python-3.12.3 torch-2.7.1+cu126 CPU (13th Gen Intel Core(TM) i7-1360P)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=custom_yolo_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/custom_yolo_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,425 parameters, 2,590,409 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 935.0Â±950.2 MB/s, size: 379.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/train/labels.cache... 35 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2834.5Â±91.4 MB/s, size: 353.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/valid/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<?, ?it/s]\n",
      "'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/train/custom_yolo_model/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/custom_yolo_model\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30         0G      1.398      3.317      1.478         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0107      0.532     0.0246    0.00868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         0G      1.245      3.282      1.442         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0124      0.641     0.0488      0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30         0G     0.9975      3.159      1.268         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0155      0.777      0.205      0.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30         0G     0.9029      3.037      1.234         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0181      0.836      0.253      0.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30         0G     0.8724      2.914      1.183         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0192      0.904      0.341      0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30         0G      0.752      2.728      1.121         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0193      0.917      0.349      0.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30         0G     0.7326      2.732      1.091         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0189      0.917      0.344      0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30         0G      0.749      2.663      1.124         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0191      0.917      0.327      0.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30         0G     0.8376      2.605      1.172         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.019       0.91      0.327      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30         0G     0.7758      2.498      1.089         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0192      0.904      0.324       0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30         0G     0.7252      2.362      1.077         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0203      0.987      0.351      0.248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30         0G     0.7321      2.269      1.131         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0204      0.987      0.387      0.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30         0G     0.6455      2.205      1.039         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0202      0.904      0.439      0.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30         0G     0.7655       2.13      1.106         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0204       0.91      0.451      0.323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30         0G     0.6855      2.003      1.027         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63     0.0215          1      0.447      0.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30         0G     0.7052      1.981      1.094         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.695      0.484      0.469      0.313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30         0G     0.6985      1.889      1.089         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63       0.93       0.28       0.54      0.348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30         0G     0.7336      1.871      1.073         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.865      0.383      0.591      0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30         0G      0.685      1.754      1.064         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.752      0.402      0.715      0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30         0G     0.7295      1.764      1.111         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.766       0.47      0.659      0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "      21/30         0G     0.6438      1.683      1.038         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.773      0.483      0.691      0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30         0G     0.7325      1.796      1.081         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.821      0.573      0.794      0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30         0G     0.5485      1.581      1.014         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.843      0.601       0.88      0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30         0G     0.7212      1.573      1.094         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.812      0.608      0.873      0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30         0G     0.4984      1.421      0.971         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.826      0.608      0.851       0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30         0G     0.5755      1.426      1.033         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.869      0.608      0.865      0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30         0G     0.5592      1.444      1.017         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.879      0.614      0.825      0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30         0G      0.554      1.327     0.9858         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.851      0.608      0.816       0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30         0G     0.5387      1.268     0.9486         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.851      0.608      0.816       0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30         0G     0.5105      1.241     0.9787         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.822      0.698      0.867      0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 0.140 hours.\n",
      "Optimizer stripped from runs/train/custom_yolo_model/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from runs/train/custom_yolo_model/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating runs/train/custom_yolo_model/weights/best.pt...\n",
      "Ultralytics 8.3.163 ðŸš€ Python-3.12.3 torch-2.7.1+cu126 CPU (13th Gen Intel Core(TM) i7-1360P)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.816      0.608      0.873      0.681\n",
      "                figure          6          8      0.682          1      0.995      0.674\n",
      "                 table          4          4          1          0      0.745       0.67\n",
      "                  text         10         51      0.766      0.824      0.878      0.698\n",
      "Speed: 1.7ms preprocess, 125.9ms inference, 0.0ms loss, 16.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/custom_yolo_model\u001b[0m\n",
      "ðŸ§™Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train the model on your custom dataset\n",
    "results = model.train(\n",
    "    data=f\"{dataset.location}/data.yaml\",  # Use the downloaded dataset path\n",
    "    epochs=30,                             # Number of training epochs\n",
    "    imgsz=640,                             # Image size\n",
    "    batch=16,                              # Batch size (adjust based on your GPU memory)\n",
    "    device='cpu',                          # GPU device (use 'cpu' if no GPU)\n",
    "    project=\"runs/train\",                  # Where to save training results\n",
    "    name=\"custom_yolo_model\"               # Name for this training run\n",
    ")\n",
    "\n",
    "print(\"ðŸ§™Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf79c0",
   "metadata": {},
   "source": [
    "### Validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1ddb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.163 ðŸš€ Python-3.12.3 torch-2.7.1+cu126 CPU (13th Gen Intel Core(TM) i7-1360P)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 4123.4Â±934.7 MB/s, size: 345.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/valid/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<?, ?it/s]\n",
      "'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10         63      0.816      0.608      0.873      0.681\n",
      "                figure          6          8      0.682          1      0.995      0.674\n",
      "                 table          4          4          1          0      0.745       0.67\n",
      "                  text         10         51      0.766      0.824      0.878      0.698\n",
      "Speed: 1.9ms preprocess, 75.6ms inference, 0.0ms loss, 15.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/custom_yolo_model2\u001b[0m\n",
      "mAP@0.5: 0.8727962365501938\n",
      "mAP@0.5:0.95: 0.6810274970425353\n",
      "Precision: 0.8160615025974295\n",
      "Recall: 0.6078431372549019\n"
     ]
    }
   ],
   "source": [
    "# Validate the trained model\n",
    "results = model.val(data=f\"{dataset.location}/data.yaml\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(f\"mAP@0.5: {results.box.map50}\")\n",
    "print(f\"mAP@0.5:0.95: {results.box.map}\")\n",
    "print(f\"Precision: {results.box.mp}\")\n",
    "print(f\"Recall: {results.box.mr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03a218",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60dd4eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found test images in: /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/test/images\n",
      "\n",
      "image 1/5 /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/test/images/arxiv_2305_02549_6_png.rf.c076d77f1864eed72b81f14c6ffae5a5.jpg: 640x480 6 texts, 51.4ms\n",
      "image 2/5 /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/test/images/arxiv_2305_02665_4_png.rf.c88c1cf5abd35af11b2a08fc179b650c.jpg: 640x480 4 texts, 56.1ms\n",
      "image 3/5 /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/test/images/arxiv_2305_03027_page_9_png.rf.3879f87b64c4184168d4a4d3fdfcd201.jpg: 640x512 1 figure, 3 texts, 53.9ms\n",
      "image 4/5 /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/test/images/arxiv_2305_03937_page_7_png.rf.4caf6ab033b4c0c82d05fca0a5308457.jpg: 640x480 1 figure, 2 texts, 68.6ms\n",
      "image 5/5 /home/terra/nopparuj/yolo11-tut/notebooks/TFT-ID-1/test/images/arxiv_2305_03981_7_png.rf.e8d001cbe005bfd0b7aacadbeefe47a5.jpg: 640x480 3 texts, 84.0ms\n",
      "Speed: 2.3ms preprocess, 62.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns/predict/test_results2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load your trained model\n",
    "model = YOLO(\"runs/train/custom_yolo_model/weights/best.pt\")\n",
    "\n",
    "# Check if test images exist in your dataset\n",
    "test_images_path = f\"{dataset.location}/test/images\"\n",
    "if os.path.exists(test_images_path):\n",
    "    print(f\"âœ… Found test images in: {test_images_path}\")\n",
    "    \n",
    "    # Run predictions on all test images\n",
    "    results = model.predict(\n",
    "        source=test_images_path,          # Path to your test images folder\n",
    "        save=True,                        # Save results\n",
    "        show=False,                       # Don't display (set to True if you want to see)\n",
    "        conf=0.1,                         # Confidence threshold\n",
    "        project=\"runs/predict\",           # Where to save predictions\n",
    "        name=\"test_results\"               # Name for this prediction run\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b67993df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Detection Summary:\n",
      "  arxiv_2305_03937_page_7_png.rf.4caf6ab033b4c0c82d05fca0a5308457.jpg: 6 detections\n",
      "    - text: 0.62\n",
      "    - text: 0.42\n",
      "    - text: 0.37\n",
      "    - text: 0.37\n",
      "    - text: 0.29\n",
      "    - text: 0.22\n",
      "  arxiv_2305_02665_4_png.rf.c88c1cf5abd35af11b2a08fc179b650c.jpg: 4 detections\n",
      "    - text: 0.52\n",
      "    - text: 0.44\n",
      "    - text: 0.34\n",
      "    - text: 0.18\n",
      "  arxiv_2305_03981_7_png.rf.e8d001cbe005bfd0b7aacadbeefe47a5.jpg: 4 detections\n",
      "    - text: 0.33\n",
      "    - text: 0.28\n",
      "    - text: 0.22\n",
      "    - figure: 0.11\n",
      "  arxiv_2305_02549_6_png.rf.c076d77f1864eed72b81f14c6ffae5a5.jpg: 3 detections\n",
      "    - text: 0.22\n",
      "    - figure: 0.20\n",
      "    - text: 0.12\n",
      "  arxiv_2305_03027_page_9_png.rf.3879f87b64c4184168d4a4d3fdfcd201.jpg: 3 detections\n",
      "    - text: 0.52\n",
      "    - text: 0.30\n",
      "    - text: 0.29\n",
      "\n",
      "ðŸŽ¯ Model classes: ['figure', 'table', 'text']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "test_images_path = f\"{dataset.location}/test/images\"\n",
    "test_image_files = glob.glob(f\"{test_images_path}/*.jpg\") + glob.glob(f\"{test_images_path}/*.png\")\n",
    "\n",
    "\n",
    "# Find prediction results directory\n",
    "pred_dirs = glob.glob(\"runs/predict/test_results_plot*\")\n",
    "if pred_dirs:\n",
    "    pred_path = max(pred_dirs, key=os.path.getmtime)\n",
    "    print(f\"ðŸ“ Using predictions from: {pred_path}\")\n",
    "else:\n",
    "    pred_path = \"runs/predict/test_results_plot\"\n",
    "\n",
    "\n",
    "print(\"\\nðŸ“Š Detection Summary:\")\n",
    "for i in range(len(results)):\n",
    "    img_name = os.path.basename(test_image_files[i])\n",
    "    result = results[i]\n",
    "    num_detections = len(result.boxes) if result.boxes is not None else 0\n",
    "    print(f\"  {img_name}: {num_detections} detections\")\n",
    "    \n",
    "    if num_detections > 0:\n",
    "        for j, box in enumerate(result.boxes):\n",
    "            confidence = box.conf.item()\n",
    "            class_id = int(box.cls.item())\n",
    "            class_name = model.names[class_id]\n",
    "            print(f\"    - {class_name}: {confidence:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model classes: {list(model.names.values())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36485845",
   "metadata": {},
   "source": [
    "Predictions:\n",
    "1. YOLO's built-in annotation when saving images\n",
    "2. YOLO uses white/light backgrounds by default for text readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b5dfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Found 5 test images\n",
      "ðŸ“ Using prediction results from: runs/predict/test_results\n",
      "ðŸ“Š Creating visualization grid: 5 rows Ã— 3 columns\n",
      "\n",
      "ðŸ“¸ Processing image 1/5: arxiv_2305_02549_6_png.rf.c076d77f1864eed72b81f14c6ffae5a5.jpg\n",
      "  Ground truth: 7 objects\n",
      "  âœ… Found prediction image\n",
      "\n",
      "ðŸ“¸ Processing image 2/5: arxiv_2305_02665_4_png.rf.c88c1cf5abd35af11b2a08fc179b650c.jpg\n",
      "  Ground truth: 5 objects\n",
      "  âœ… Found prediction image\n",
      "\n",
      "ðŸ“¸ Processing image 3/5: arxiv_2305_03027_page_9_png.rf.3879f87b64c4184168d4a4d3fdfcd201.jpg\n",
      "  Ground truth: 8 objects\n",
      "  âœ… Found prediction image\n",
      "\n",
      "ðŸ“¸ Processing image 4/5: arxiv_2305_03937_page_7_png.rf.4caf6ab033b4c0c82d05fca0a5308457.jpg\n",
      "  Ground truth: 8 objects\n",
      "  âœ… Found prediction image\n",
      "\n",
      "ðŸ“¸ Processing image 5/5: arxiv_2305_03981_7_png.rf.e8d001cbe005bfd0b7aacadbeefe47a5.jpg\n",
      "  Ground truth: 6 objects\n",
      "  âœ… Found prediction image\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ’¾ Figure saved as 'runs/predict/test_comparison.png'\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š GROUND TRUTH SUMMARY\n",
      "============================================================\n",
      "Total Ground Truth Objects: 34\n",
      "\n",
      "Class-wise Ground Truth Count:\n",
      "  figure  :  6 objects\n",
      "  table   :  7 objects\n",
      "  text    : 21 objects\n",
      "\n",
      "âœ… Visualization complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "# Configure matplotlib for Jupyter notebook\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "\n",
    "# Class names and colors for visualization\n",
    "class_names = ['figure', 'table', 'text']\n",
    "colors = ['red', 'blue', 'green']\n",
    "color_map = {i: colors[i] for i in range(len(class_names))}\n",
    "\n",
    "def parse_yolo_label(label_file, img_width, img_height):\n",
    "    \"\"\"Parse YOLO format label file and return bounding boxes\"\"\"\n",
    "    boxes = []\n",
    "    if os.path.exists(label_file):\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center = float(parts[1]) * img_width\n",
    "                    y_center = float(parts[2]) * img_height\n",
    "                    width = float(parts[3]) * img_width\n",
    "                    height = float(parts[4]) * img_height\n",
    "                    \n",
    "                    # Convert to top-left corner format\n",
    "                    x1 = x_center - width/2\n",
    "                    y1 = y_center - height/2\n",
    "                    x2 = x_center + width/2\n",
    "                    y2 = y_center + height/2\n",
    "                    \n",
    "                    boxes.append({\n",
    "                        'class_id': class_id,\n",
    "                        'class_name': class_names[class_id],\n",
    "                        'bbox': [x1, y1, x2, y2],\n",
    "                        'confidence': 1.0  # Ground truth has 100% confidence\n",
    "                    })\n",
    "    return boxes\n",
    "\n",
    "def load_prediction_results():\n",
    "    \"\"\"Load existing prediction results from runs/predict folder\"\"\"\n",
    "    pred_dirs = glob.glob(\"runs/predict/test_results*\")\n",
    "    if not pred_dirs:\n",
    "        print(\"âŒ No prediction results found! Please run predictions first.\")\n",
    "        return None\n",
    "    \n",
    "    # Use the latest prediction directory\n",
    "    pred_dir = max(pred_dirs, key=os.path.getmtime)\n",
    "    print(f\"ðŸ“ Using prediction results from: {pred_dir}\")\n",
    "    \n",
    "    # Find prediction images\n",
    "    pred_images = glob.glob(f\"{pred_dir}/*.jpg\") + glob.glob(f\"{pred_dir}/*.png\")\n",
    "    if not pred_images:\n",
    "        print(f\"âŒ No prediction images found in {pred_dir}\")\n",
    "        return None\n",
    "    \n",
    "    return pred_dir\n",
    "\n",
    "def draw_boxes(ax, boxes, title, img_width, img_height):\n",
    "    \"\"\"Draw bounding boxes on the image\"\"\"\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, img_width)\n",
    "    ax.set_ylim(img_height, 0)  # Flip y-axis for image coordinates\n",
    "    \n",
    "    for box in boxes:\n",
    "        class_id = box['class_id']\n",
    "        class_name = box['class_name']\n",
    "        bbox = box['bbox']\n",
    "        confidence = box['confidence']\n",
    "        \n",
    "        x1, y1, x2, y2 = bbox\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Draw rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height, \n",
    "            linewidth=2, \n",
    "            edgecolor=color_map[class_id], \n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        label = f\"{class_name}: {confidence:.2f}\"\n",
    "        ax.text(x1, y1-5, label, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color_map[class_id], alpha=0.7),\n",
    "                fontsize=10, color='white', weight='bold')\n",
    "\n",
    "# Get test images and labels\n",
    "test_images_path = f\"{dataset.location}/test/images\"\n",
    "test_labels_path = f\"{dataset.location}/test/labels\"\n",
    "\n",
    "test_image_files = sorted(glob.glob(f\"{test_images_path}/*.jpg\") + glob.glob(f\"{test_images_path}/*.png\"))\n",
    "print(f\"ðŸ“ Found {len(test_image_files)} test images\")\n",
    "\n",
    "# Load prediction results\n",
    "pred_dir = load_prediction_results()\n",
    "if pred_dir is None:\n",
    "    print(\"âŒ Cannot proceed without prediction results!\")\n",
    "    exit()\n",
    "\n",
    "# Create comparison plots with smaller, more manageable size\n",
    "fig_width = 15\n",
    "fig_height = 4 * len(test_image_files)\n",
    "fig, axes = plt.subplots(len(test_image_files), 3, figsize=(fig_width, fig_height))\n",
    "\n",
    "if len(test_image_files) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "print(f\"ðŸ“Š Creating visualization grid: {len(test_image_files)} rows Ã— 3 columns\")\n",
    "\n",
    "for i, img_path in enumerate(test_image_files):\n",
    "    print(f\"\\nðŸ“¸ Processing image {i+1}/{len(test_image_files)}: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Load original image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Get image filename without extension for label file\n",
    "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    label_file = os.path.join(test_labels_path, f\"{img_name}.txt\")\n",
    "    \n",
    "    # Parse ground truth labels\n",
    "    gt_boxes = parse_yolo_label(label_file, img_width, img_height)\n",
    "    print(f\"  Ground truth: {len(gt_boxes)} objects\")\n",
    "    \n",
    "    # Find corresponding prediction image\n",
    "    pred_image_path = os.path.join(pred_dir, os.path.basename(img_path))\n",
    "    if os.path.exists(pred_image_path):\n",
    "        pred_img = Image.open(pred_image_path).convert('RGB')\n",
    "        print(f\"  âœ… Found prediction image\")\n",
    "    else:\n",
    "        pred_img = img  # Use original if prediction not found\n",
    "        print(f\"  âš ï¸ Prediction image not found, using original\")\n",
    "    \n",
    "    # Plot original image\n",
    "    axes[i][0].imshow(img)\n",
    "    axes[i][0].set_title(f\"Original Image\\n{os.path.basename(img_path)}\", fontsize=12)\n",
    "    axes[i][0].axis('off')\n",
    "    \n",
    "    # Plot ground truth\n",
    "    axes[i][1].imshow(img)\n",
    "    draw_boxes(axes[i][1], gt_boxes, f\"Ground Truth\\n{len(gt_boxes)} objects\", img_width, img_height)\n",
    "    axes[i][1].axis('off')\n",
    "    \n",
    "    # Plot prediction image (with annotations already drawn by YOLO)\n",
    "    axes[i][2].imshow(pred_img)\n",
    "    axes[i][2].set_title(f\"Predictions\\n(with annotations)\", fontsize=12)\n",
    "    axes[i][2].axis('off')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [patches.Patch(color=color_map[i], label=class_names[i]) for i in range(len(class_names))]\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Test Images: Ground Truth vs Predictions', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# Save the figure to ensure it's created\n",
    "plt.savefig('runs/predict/test_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¾ Figure saved as 'runs/predict/test_comparison.png'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary statistics for ground truth\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š GROUND TRUTH SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_gt = 0\n",
    "class_gt_counts = {name: 0 for name in class_names}\n",
    "\n",
    "for i, img_path in enumerate(test_image_files):\n",
    "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    label_file = os.path.join(test_labels_path, f\"{img_name}.txt\")\n",
    "    \n",
    "    # Count ground truth objects\n",
    "    img = Image.open(img_path)\n",
    "    img_width, img_height = img.size\n",
    "    gt_boxes = parse_yolo_label(label_file, img_width, img_height)\n",
    "    \n",
    "    total_gt += len(gt_boxes)\n",
    "    \n",
    "    # Count by class\n",
    "    for box in gt_boxes:\n",
    "        class_gt_counts[box['class_name']] += 1\n",
    "\n",
    "print(f\"Total Ground Truth Objects: {total_gt}\")\n",
    "\n",
    "print(f\"\\nClass-wise Ground Truth Count:\")\n",
    "for class_name in class_names:\n",
    "    gt_count = class_gt_counts[class_name]\n",
    "    print(f\"  {class_name:8}: {gt_count:2d} objects\")\n",
    "\n",
    "print(\"\\nâœ… Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07f1d94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Re-running predictions to get detection data...\n",
      "âœ… Got prediction results for 5 images\n",
      "\n",
      "ðŸ“Š Prediction Summary:\n",
      "  arxiv_2305_02549_6_png.rf.c076d77f1864eed72b81f14c6ffae5a5.jpg: 6 detections\n",
      "    - text: 0.62\n",
      "    - text: 0.42\n",
      "    - text: 0.37\n",
      "    - text: 0.37\n",
      "    - text: 0.29\n",
      "    - text: 0.22\n",
      "  arxiv_2305_02665_4_png.rf.c88c1cf5abd35af11b2a08fc179b650c.jpg: 4 detections\n",
      "    - text: 0.52\n",
      "    - text: 0.44\n",
      "    - text: 0.34\n",
      "    - text: 0.18\n",
      "  arxiv_2305_03027_page_9_png.rf.3879f87b64c4184168d4a4d3fdfcd201.jpg: 4 detections\n",
      "    - text: 0.33\n",
      "    - text: 0.28\n",
      "    - text: 0.22\n",
      "    - figure: 0.11\n",
      "  arxiv_2305_03937_page_7_png.rf.4caf6ab033b4c0c82d05fca0a5308457.jpg: 3 detections\n",
      "    - text: 0.22\n",
      "    - figure: 0.20\n",
      "    - text: 0.12\n",
      "  arxiv_2305_03981_7_png.rf.e8d001cbe005bfd0b7aacadbeefe47a5.jpg: 3 detections\n",
      "    - text: 0.52\n",
      "    - text: 0.30\n",
      "    - text: 0.29\n",
      "\n",
      "Total Predictions: 20\n",
      "  figure: 2 objects\n",
      "  table: 0 objects\n",
      "  text: 18 objects\n",
      "\n",
      "ðŸ” Creating confusion matrix with IoU threshold: 0.5\n",
      "ðŸ“¸ Processing 1/5: arxiv_2305_02549_6_png.rf.c076d77f1864eed72b81f14c6ffae5a5.jpg\n",
      "  Ground truth: 7 objects | Predictions: 6 objects\n",
      "ðŸ“¸ Processing 2/5: arxiv_2305_02665_4_png.rf.c88c1cf5abd35af11b2a08fc179b650c.jpg\n",
      "  Ground truth: 5 objects | Predictions: 4 objects\n",
      "ðŸ“¸ Processing 3/5: arxiv_2305_03027_page_9_png.rf.3879f87b64c4184168d4a4d3fdfcd201.jpg\n",
      "  Ground truth: 8 objects | Predictions: 4 objects\n",
      "ðŸ“¸ Processing 4/5: arxiv_2305_03937_page_7_png.rf.4caf6ab033b4c0c82d05fca0a5308457.jpg\n",
      "  Ground truth: 8 objects | Predictions: 3 objects\n",
      "ðŸ“¸ Processing 5/5: arxiv_2305_03981_7_png.rf.e8d001cbe005bfd0b7aacadbeefe47a5.jpg\n",
      "  Ground truth: 6 objects | Predictions: 3 objects\n",
      "ðŸ’¾ Confusion matrix saved as 'runs/predict/confusion_matrix.png'\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š CONFUSION MATRIX ANALYSIS\n",
      "============================================================\n",
      "Total Ground Truth Objects: 34\n",
      "Total Predicted Objects: 20\n",
      "Total Matches (IoU â‰¥ 0.5): 19\n",
      "Overall Precision: 0.950\n",
      "Overall Recall: 0.559\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class      TP   FP   FN   Precision  Recall     F1-Score  \n",
      "----------------------------------------------------------------------\n",
      "figure     2    0    4    1.000      0.333      0.500     \n",
      "table      0    0    7    0.000      0.000      0.000     \n",
      "text       17   1    4    0.944      0.810      0.872     \n",
      "\n",
      "âœ… Confusion matrix analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix - Using Actual Model Results, not searching for non-existant predicted label files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Set up matplotlib\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Use non-interactive backend\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Variables from previous cells\n",
    "class_names = ['figure', 'table', 'text']\n",
    "test_images_path = f\"{dataset.location}/test/images\"\n",
    "test_labels_path = f\"{dataset.location}/test/labels\"\n",
    "test_image_files = sorted(glob.glob(f\"{test_images_path}/*.jpg\") + glob.glob(f\"{test_images_path}/*.png\"))\n",
    "\n",
    "# Re-run predictions to get actual results data (not just images)\n",
    "print(\"ðŸ”„ Re-running predictions to get detection data...\")\n",
    "\n",
    "# Load the trained model\n",
    "model = YOLO(\"runs/train/custom_yolo_model/weights/best.pt\")\n",
    "\n",
    "# Run predictions on test images to get actual results\n",
    "results = model.predict(\n",
    "    source=test_images_path,\n",
    "    save=False,  # Don't save images, just get results\n",
    "    show=False,\n",
    "    conf=0.1,  # Same confidence threshold as original\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Got prediction results for {len(results)} images\")\n",
    "\n",
    "# Print prediction summary to match test results\n",
    "print(\"\\nðŸ“Š Prediction Summary:\")\n",
    "total_predictions = 0\n",
    "class_pred_counts = {name: 0 for name in class_names}\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    img_name = os.path.basename(test_image_files[i])\n",
    "    num_detections = len(result.boxes) if result.boxes is not None else 0\n",
    "    total_predictions += num_detections\n",
    "    print(f\"  {img_name}: {num_detections} detections\")\n",
    "    \n",
    "    if num_detections > 0:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls.item())\n",
    "            class_name = class_names[class_id]\n",
    "            class_pred_counts[class_name] += 1\n",
    "            confidence = box.conf.item()\n",
    "            print(f\"    - {class_name}: {confidence:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal Predictions: {total_predictions}\")\n",
    "for class_name in class_names:\n",
    "    count = class_pred_counts[class_name]\n",
    "    print(f\"  {class_name}: {count} objects\")\n",
    "\n",
    "# Helper functions\n",
    "def parse_yolo_label(label_file, img_width, img_height):\n",
    "    \"\"\"Parse YOLO format label file and return bounding boxes\"\"\"\n",
    "    boxes = []\n",
    "    if os.path.exists(label_file):\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center = float(parts[1]) * img_width\n",
    "                    y_center = float(parts[2]) * img_height\n",
    "                    width = float(parts[3]) * img_width\n",
    "                    height = float(parts[4]) * img_height\n",
    "                    \n",
    "                    # Convert to top-left corner format\n",
    "                    x1 = x_center - width/2\n",
    "                    y1 = y_center - height/2\n",
    "                    x2 = x_center + width/2\n",
    "                    y2 = y_center + height/2\n",
    "                    \n",
    "                    boxes.append({\n",
    "                        'class_id': class_id,\n",
    "                        'class_name': class_names[class_id],\n",
    "                        'bbox': [x1, y1, x2, y2],\n",
    "                        'confidence': 1.0  # Ground truth has 100% confidence\n",
    "                    })\n",
    "    return boxes\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) of two bounding boxes\"\"\"\n",
    "    # box format: [x1, y1, x2, y2]\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "    \n",
    "    # Calculate intersection coordinates\n",
    "    x1_inter = max(x1_1, x1_2)\n",
    "    y1_inter = max(y1_1, y1_2)\n",
    "    x2_inter = min(x2_1, x2_2)\n",
    "    y2_inter = min(y2_1, y2_2)\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    if x2_inter <= x1_inter or y2_inter <= y1_inter:\n",
    "        intersection = 0\n",
    "    else:\n",
    "        intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
    "    \n",
    "    # Calculate areas of both boxes\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    \n",
    "    # Calculate union area\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union\n",
    "\n",
    "def match_predictions_to_ground_truth(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Match predictions to ground truth boxes using IoU threshold\"\"\"\n",
    "    matches = []\n",
    "    matched_gt = set()\n",
    "    matched_pred = set()\n",
    "    \n",
    "    # Sort predictions by confidence (highest first)\n",
    "    pred_boxes_sorted = sorted(enumerate(pred_boxes), key=lambda x: x[1]['confidence'], reverse=True)\n",
    "    \n",
    "    for pred_idx, pred_box in pred_boxes_sorted:\n",
    "        if pred_idx in matched_pred:\n",
    "            continue\n",
    "            \n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        # Find best matching ground truth box\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "            if gt_idx in matched_gt:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(pred_box['bbox'], gt_box['bbox'])\n",
    "            \n",
    "            if iou > best_iou and iou >= iou_threshold:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_gt_idx != -1:\n",
    "            matches.append({\n",
    "                'pred_idx': pred_idx,\n",
    "                'gt_idx': best_gt_idx,\n",
    "                'pred_class': pred_box['class_id'],\n",
    "                'gt_class': gt_boxes[best_gt_idx]['class_id'],\n",
    "                'iou': best_iou,\n",
    "                'confidence': pred_box['confidence']\n",
    "            })\n",
    "            matched_gt.add(best_gt_idx)\n",
    "            matched_pred.add(pred_idx)\n",
    "    \n",
    "    return matches, matched_gt, matched_pred\n",
    "\n",
    "def create_confusion_matrix(iou_threshold=0.5):\n",
    "    \"\"\"Create confusion matrix for object detection results\"\"\"\n",
    "    print(f\"\\nðŸ” Creating confusion matrix with IoU threshold: {iou_threshold}\")\n",
    "    \n",
    "    # Initialize confusion matrix\n",
    "    num_classes = len(class_names)\n",
    "    confusion_matrix = np.zeros((num_classes + 1, num_classes + 1), dtype=int)\n",
    "    \n",
    "    # Class names with background\n",
    "    class_names_with_bg = class_names + ['background']\n",
    "    \n",
    "    # Statistics\n",
    "    total_gt = 0\n",
    "    total_pred = 0\n",
    "    total_matches = 0\n",
    "    \n",
    "    # Process each test image\n",
    "    for i, img_path in enumerate(test_image_files):\n",
    "        print(f\"ðŸ“¸ Processing {i+1}/{len(test_image_files)}: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        # Load image dimensions\n",
    "        img = Image.open(img_path)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # Get ground truth boxes\n",
    "        img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        label_file = os.path.join(test_labels_path, f\"{img_name}.txt\")\n",
    "        gt_boxes = parse_yolo_label(label_file, img_width, img_height)\n",
    "        \n",
    "        # Get prediction boxes from model results\n",
    "        result = results[i]\n",
    "        pred_boxes = []\n",
    "        \n",
    "        if result.boxes is not None:\n",
    "            for box in result.boxes:\n",
    "                # Get box coordinates (already in pixel coordinates)\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                class_id = int(box.cls.item())\n",
    "                confidence = box.conf.item()\n",
    "                \n",
    "                pred_boxes.append({\n",
    "                    'class_id': class_id,\n",
    "                    'class_name': class_names[class_id],\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        \n",
    "        total_gt += len(gt_boxes)\n",
    "        total_pred += len(pred_boxes)\n",
    "        \n",
    "        print(f\"  Ground truth: {len(gt_boxes)} objects | Predictions: {len(pred_boxes)} objects\")\n",
    "        \n",
    "        # Match predictions to ground truth\n",
    "        matches, matched_gt, matched_pred = match_predictions_to_ground_truth(\n",
    "            gt_boxes, pred_boxes, iou_threshold\n",
    "        )\n",
    "        \n",
    "        total_matches += len(matches)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        # True positives and false positives\n",
    "        for match in matches:\n",
    "            gt_class = match['gt_class']\n",
    "            pred_class = match['pred_class']\n",
    "            confusion_matrix[gt_class, pred_class] += 1\n",
    "        \n",
    "        # False positives (unmatched predictions)\n",
    "        for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "            if pred_idx not in matched_pred:\n",
    "                pred_class = pred_box['class_id']\n",
    "                confusion_matrix[num_classes, pred_class] += 1  # background -> predicted class\n",
    "        \n",
    "        # False negatives (unmatched ground truth)\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "            if gt_idx not in matched_gt:\n",
    "                gt_class = gt_box['class_id']\n",
    "                confusion_matrix[gt_class, num_classes] += 1  # true class -> background\n",
    "    \n",
    "    return confusion_matrix, class_names_with_bg, total_gt, total_pred, total_matches\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, class_names_with_bg, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix with nice formatting\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        confusion_matrix,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names_with_bg,\n",
    "        yticklabels=class_names_with_bg,\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Add text explanation\n",
    "    plt.figtext(0.02, 0.02, \n",
    "                'Note: \"background\" represents unmatched predictions (false positives) or ground truth (false negatives)',\n",
    "                fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix, class_names_with_bg, total_gt, total_pred, total_matches = create_confusion_matrix(iou_threshold=0.5)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(confusion_matrix, class_names_with_bg, \"Object Detection Confusion Matrix (IoU â‰¥ 0.5)\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('runs/predict', exist_ok=True)\n",
    "plt.savefig('runs/predict/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "print(\"ðŸ’¾ Confusion matrix saved as 'runs/predict/confusion_matrix.png'\")\n",
    "\n",
    "# Calculate and display metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total Ground Truth Objects: {total_gt}\")\n",
    "print(f\"Total Predicted Objects: {total_pred}\")\n",
    "print(f\"Total Matches (IoU â‰¥ 0.5): {total_matches}\")\n",
    "print(f\"Overall Precision: {total_matches/total_pred:.3f}\" if total_pred > 0 else \"Overall Precision: N/A\")\n",
    "print(f\"Overall Recall: {total_matches/total_gt:.3f}\" if total_gt > 0 else \"Overall Recall: N/A\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "print(f\"{'Class':<10} {'TP':<4} {'FP':<4} {'FN':<4} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    tp = confusion_matrix[i, i]  # True positives\n",
    "    fp = confusion_matrix[-1, i]  # False positives (background -> predicted as this class)\n",
    "    fn = confusion_matrix[i, -1]  # False negatives (this class -> predicted as background)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"{class_name:<10} {tp:<4} {fp:<4} {fn:<4} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Confusion matrix analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
